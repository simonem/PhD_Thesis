
# Evaluation

This chapter provides an evaluation of the contributions of the thesis with respect to research questions. In addition, validity threats are discussed in the chapter. 

## Evaluation of Research questions

### MRQ: How technology can introduce new opportunities to promote reflection on work experiences?

The main research question is answered by Contribution 1. Using *crisis training* as case study for design research, it is demonstrated how conceptual tools from theory in CSRL can be implemented in a suite of technology applications: WATCHiT, CroMAR, Trainer and Don’t Panic (DoomTown). Each application assists a specific activity which the CSRL model has identified to be relevant for reflection: plan and do work, monitor work, initiate and conduct reflection session. Two or more applications can be configured to work together in an ecology, in order to address the interrelated nature of CSRL activities in terms of sharing data and reflection outcomes. In this way it is also addressed the need for supporting the extremely vast range of crisis-work scenarios workers train for by means of loosely coupled, modular applications.

### RQ1: How ~~sensor-based~~ computer interfaces can enable pervasive and unobtrusive data collection at work?

This question is answered by Contribution 2 and Contribution 3. C2 identified a set of challenges for designing data capturing tools, focusing on *what* data to collect and *how* to collect it. One of the identified challenges, the need of intuitive and hands-free user interfaces for capturing user-submitted qualitative information, has been further addressed by C3 with the design of a novel interaction technique.  

### RQ2: What interaction modalities with digital information can help in engaging reflection processes?

This question is answered by Contribution 3. It is found out that theory in the field of tangible, embodied and embedded computing can drive the design of interaction modalities with the digital world either for *capture work experiences* and to *generate work experiences*. The specific paradigms put in use are mobile augmented reality, body-centric interaction and token+constraint interaction. Once more it is not a single interaction modality that has been proven to engage reflection but rather a mix of different technology-assisted experiences. 

### RQ3: How technology tools for supporting reflection can be rapid prototyped?

This question is answered by Contribution 4. It is found out that prototyping tools for supporting reflection require a wide range of skills ranging from software and electronic engineering and industrial design. Yet a toolbox for assisting in full the work of designers and engineers in implementing functional requirements in prototypes couldn’t be identified. Despite so recent advances in open source hardware and software, digital manufacturing and technology developed by the author (P7) allowed the author to build working prototypes robust enough to undergo evaluations during simulated crisis work. Still the production of prototypes for supporting reflection require a lot of resources in terms of time, money and skills not usually available among the research communities this PhD work belongs to.

## Evaluation of research approach

The research approach taken has a number of limitations. In this section the overall research approach is evaluated and validity issues are discussed.

### Conclusion validity

Conclusion validity question whether the method used shows a relationship between the variables that are studied.

In this work the efficacy in supporting reflective learning for the technology tools developed was evaluated (as proof of validity for C1). Also due to the novel nature of the user interaction paradigms for the applications developed, technology acceptances tests and usability evaluations have been performed. 

Considering the design research approach taken and the thus the number of human factors involved, methods from qualitative research were used in the evaluations. The methods used included observations, semi-structured interviews, surveys and questionnaires. The adoption of the MIRROR Evaluation Toolbox [@Renner:v4nLmwOk], which provided questionnaires to measure reflective learning at work, straighten the conclusion validity of the research. These questionaries are built on the Kirkpatrick framework [REF] and have been developed through an extensive survey of literature on reflective learning and in cooperation with participants from different workplace settings. 

The number of participants in the studies added up to 56 crisis field workers, 1 disaster manager, 8 IT students and 4 HCI experts. Among the field workers 5 participated in more than one study. The size of data samples used for statistical analysis is therefore limited and it weakens the conclusion validity, especially for the evaluation studies for the applications targeting disaster managers (CroMAR). Furthermore most of evaluation studies lasted only for a few hours. It is therefore not possible to asses long-term learning effects for the solutions developed.

Running user studies in crisi org. requires lot of work to 

### Internal validity

Internal validity questions whether the method used demonstrates a casual relationship between two variables or not.

By running field studies during rescue work simulated for training purposes and arranged by external organisations, most variables couldn’t be controlled. Access to workers and locations where the work is performed was restrained to avoid obstructing activities and put the training objectives of the events at risk. Despite the good will of simulations attendees and organisers, often it wasn’t possible to fully attend to the evaluation protocol agreed due to both planned and unplanned unpredictability of such events.  

Yet during field studies, lack of control was traded for realism. Drills attended recreated working conditions as close as possible to real emergencies. Participants were unaware of the type of emergency to face until the very last, human mistakes and failure of technical systems were re-created on purpose by the drill manager. Finally, volunteers acting as injured were made-up with fake wounds and instructed to behave accordingly (e.g. panicking). Considering the very situated nature of crisis work, the widespread layout of activities in space, the coexistence of multiple roles and organisations, choosing field studies over an experimental strategy has strengthen the internal validity of the work. Giving for granted that running field studies during real crisis work is not feasible due to safety and legal issues.

### External validity

External validity questions whether the relationship found can be generalised to other settings.

Given the characteristics of the sample of population the research has been evaluated with, it is not possible to draw conclusions about generalisations of results, both within the crisis domain and to other settings. 

First participants in the studies are all affiliated to the same emergency association; the design research has been case studied on the specific needs of that organisation. Although literature reviews have shown that technology requirements found in our user studies are common to other organisations in different countries, it is not given to know how the results from this work could be accepted by other organisations. This weakens the results of research. Contacts with other emergency organisations have been established in order to evaluate generalisation of results.

Second it hasn’t been thoroughly investigated how results from this PhD could be translated to work practices different than the crisis domain. The novel interaction techniques and the rapid prototyping approach developed have been employed to create games for training workers in dementia care homes for better care, during a research visit abroad. The same approach drove the design and implementation of tangible interface to promote user engagement and reflection about urban-mobility data , as part of a second research visit abroad.

### Reliability

Reliability is the extend to which another researcher would find  the same answers.

Most of the data were only observed, recorded and analysed by one person, which is subject to interpretation bias. However some of the data were analysed by two or more people. Further, different researchers have been responsible for different studies. 

