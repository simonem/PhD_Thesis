# Sensing-based interaction

The design of systems to support reflection on crisis work experiences benefits from research in the field of HCI and sensing-based interaction [@Benford:2005bo]. Sensing-based interfaces is rather a broad term referring to user interfaces that rely on sensor technology to make interaction between people and computers more intuitive and effective [@Zhai:2005jm]. Sensor interfaces allows for post-WIMP [@VanDam:1997tz] interaction paradigms, to design user interfaces not relying on traditional **W**indows, **I**cons, **M**ouse and **P**ointers methapors. Instead, sensing-based interfaces promote “embodied interaction, tangible manipulation, physical representation of data and embeddedness in real space” [@Hornecker:2006uq]. 

According with Rogers and Muller [-@Rogers:2006te] sensing-based interaction allows for designing systems capable to deliver relevant information at appropriate times, which is critical to trigger and sustain reflection; and enable “hands-free control”, which is fundamental for unobtrusively capturing data in action. *Embodied* and *Tangible* are characterising tracts of sensing-based interfaces that will be investigated in the next section.

## Tangible interfaces and Embodied interaction

Embodied interaction, as defined by Dourish [@Dourish:2001vc], is a collection of recent trends emerged in HCI, relying on the common ground to provide a natural, native user interaction with digital information. Embodied interaction makes an enormous shift from previous user interaction paradigm. Moving from time to space, it takes the interaction “off the screen” into the real world [@Dourish:2001vc]; distributing inputs in space, de-sequentialising interaction and reducing the gap between where the information is created where it is accessed. The interactional media, with its affordances, is the interface: “By treating the body of the device as part of the user interface —an embodied user interface— we can go beyond the manipulation of a GUI and allow the user to really directly manipulate an integrated physical-virtual device” [@Fishkin:2000df] 

*Tangible user interfaces* are computer interfaces in which technology is embedded into physical objects and spaces, enabling an *embodied interaction* with digital information. In this picture, unlike GUIs which manipulate virtual elements (e.g. icons) with the aid of keyboard mouse, TUIs integrates both representation and control of computation into physical artefacts [@krumm2009ubiquitous]. This approach allows system designers to be free to experiment with new type of metaphors, taking advantage of users’ physical skills and providing interfaces which exploit people’s knowledge with the everyday, non-digital, world [@Jacob:2008vm]. Since the ways the manipulation of physical media profoundly differs from the manipulation of the digital [@Terrenghi:2007uv], metaphors adopted for the digital world need to be redesigned to meet physical affordances. Indeed, TUIs as well as other sensing-based interfaces poses new challenges to designers [@Bellotti:2002wg]. Those challenges were also further elaborated by extended by Marquardt and Greenberg [-@Marquardt:2012tg] Several terms have been used to characterise systems of tangible interfaces, as tangible, graspables, tokens, containers, phicons, tangible bits; in the following I will call them tangibles. 

Tangibles are part of the broad field known as Ubiquitous Computing, widely attributed to the work of Mark Weiser. In his pioneering article on Scientific American in 1991 [@weiser1991computer] he envisioned a close coupling between the digital and the physical, to the extent that the technology “disappears into the fabric of the everyday life”.

### Frameworks

Over the recent years several research works have proposed frameworks either to characterise existing systems or to provide  opportunities and guidelines to support the design of new systems of tangibles. In 2000 Ullmer and Ishii [-@Ullmer:2000vf] took the first steps in investigating the design of tangibles. They defined TUIs as systems that give physical form to digital information, a research branded as “tangible bits” [@Ishii:2008fh]; and categorised them in spatial, relational, constructive and mixed systems. They presented a conceptual framework and interaction model called MCRit.  MCRit, an abbreviation for Model-Control-Representation (tangible and intangible), adapts the Model-View-Controller (MVC) model of GUI-based interaction to the design of tangibles. Besides MCRit highlights the “seamless integration of control and representation” characteristic of tangibles, it redefines the capability of the interface to provide information as a balance between its physical (the object’s shape and affordances) and a intangible representations (e.g. computer graphics and sounds)(Figure x). For example augmenting physical objects with video-projections and sounds in order to extend the static representation of an object with an intangible, dynamic one.

![Tangible and intangible representations of TUI, figure from [@Ishii:2008fh]](imgs/mcrit.png)

In fact one of the main pitfall of TUIs is that while 
GUIs serve as generic purpose interface by allowing multiple kinds of tasks, defined by the software, TUIs serve as special purposes interfaces, each one tailored on a specific set of 
actions (defined by physical affordances and constrains). A tangible interface can hardly be adapted to work in a context that differs from the one it has been designed for. It is what is defined as the trade-off between Reality and Versatility [@Jacob:2008vm]: it trades the capability of a system of doing many different tasks (like browsing photos, writing a document) with the possibility to accomplish only one single task with a higher level of realism or simplicity. 

Following the work of Ullmer and Ishii other works have looked at TUIs from other perspectives. Jacob et al. [-@Jacob:2008vm] identify four themes of interaction with the real world that can be leveraged for the design of TUIs. Hornecker and Buur [-@Hornecker:2006uq] present four topics to be considered in scenarios where tangible interaction have social aspects. Fishking [@Fishkin:2004uv] presents an aggregated perspective on other frameworks categorising tangible systems as a continuum of level of embodiment and metaphor they provide. Finally Ullmer et al. envision TUIs as a systems of *tokens* and *constraints*. The former are discrete physical objects that represent digital information, the latter mechanical or visual confining regions that are mapped to digital operations. By the interaction phases of association and manipulation of tokens within a system of constraints it is possible to map physical actions to a set of computational operations in a grammar of ways; for example the presence or absence of a token in a constrained area could be easily digitalised in binary information to trigger a digital operation. For a literature of other frameworks see [@Mazalek:2009uy] and [@Shaer:2009fx]. 

### Tangibles and learning

Although the relation between tangibles and experiential learning hasn’t been thoroughly investigated, several works have shown that TUIs might be beneficial for learning [@Marshall:2007dr]; for a review see [@omalley:hal-00190328]. Although those works often focus on applications of TUIs for children and classroom environments, there are evidences that TUIs have possible benefits on learning on a broader scope. Those benefits include the support to more natural [@Terrenghi:2005gq] and situated [@Klemmer:2006ez] learning , increased reflection and engagement [@Rogers:2006te] due to the link between physical action and digital feedbacks. Moreover TUIs foster collaboration [@Rogers:2003tt], in which they increase visibility of others’ actions and allow for concurrent interaction.

### Tangibles and crisis learning

Theoretical tools adapted from the presented frameworks have driven the design of embodied interaction systems in P2, P3, P4, P6.

The system presented in P1 is based on mobile augmented reality (MAR). Whereas *virtual reality* aims at immersing the user into a world of virtual artefacts, *augmented reality* aims at enhancing (not re-creating) the real world with information from the digital space which technology allows to overlay to the physical space. Besides augmented reality is not always considered as an embodied interaction paradigm, the system developed in P1  requires the user to move the interface in the physical space in order to interact with the system. This interaction technique, shares with tangible systems the goal of supporting an exploration of the physical space. In general the boundaries between ubiquitous computing, embodied interaction, tangible interfaces and augmented reality are blurry. Dourish [-@Dourish:2001vc], for example, refers to them all as tangible computing.
