# Sensing-based interaction

[Revision 1]

This chapter reviews HCI theories to inform the design of computer interfaces to *capture*, *re-create* and *generate* crisis training experiences. User studies conducted by the author have identified core requirements for the technology to support each of the three stages. Focus groups with workers and experts highlighted the need for simple and intuitive user interfaces. While for the capture stage the sought objective is to minimise distraction, to not put rescue work at risk; during re-creation and generation of experiences the goal is to provide user experiences which are situated, and highly interactive. 

During the field studies the intrinsic physicality of crisis work was very visibile. Contrarily to traditional office work, crisis work is often performed in harsh environments; workers deal with fire extinguishers, medical equipments, they break into crashed cars and houses falling apart to rescue people. The physical environment is at the same time source of dangers, challenges and learning opportunities. Most of the activities are collaborative, e.g. carrying someone injured on a stretcher. Interviews about workers everyday use of ICT technology, revealed little acquaintance with traditional user interfaces (point-and-click, touch-based, etc..).

When it came to design computer interfaces to support reflection for this very peculiar target group, it seemed to me clever to preserve some extent of *physicality* into their user-experiences with technology, with some distinction. While to assist the *capture* of work experiences the goal is to create technology which is unobtrusive because it blends into the (physical) work practice to become part of it; to support *re-creation* and *generation* of experiences technology should enable physical exploration of space and tangible interaction with digital information.

After surveying HCI literature for theoretical frameworks to facilitate integrating *physicality* into computer interfaces I focused on the aspects of *embodiment* and *tangibility*. Those are characterising traits of sensing-based interfaces [@Benford:2005bo]. 

Sensing-based interfaces is rather a broad term referring to user interfaces that rely on sensor technology to make interaction between people and computers more intuitive and effective [@Zhai:2005jm]. They allow for post-**WIMP** [@VanDam:1997tz] interaction paradigms, to design user interfaces not relying on traditional **W**indows, **I**cons, **M**ouse and **P**ointers methapors. Instead, sensing-based interfaces promote “embodied interaction, tangible manipulation, physical representation of data and embeddedness in the real space” [@Hornecker:2006uq]. According with Rogers and Muller [-@Rogers:2006te] sensing-based interaction allows for designing systems capable to deliver relevant information at appropriate times, which is critical to trigger and sustain reflection; and enable “hands-free control”, which is fundamental for unobtrusively capturing data in action. 

Theoretical tools to include *embodiment* and *tangibility* into user interfaces are reviewed in the next section.

## Tangible interfaces and Embodied interaction

Embodied interaction, as defined by Dourish [@Dourish:2001vc], is a collection of recent trends emerged in HCI, relying on the common ground to provide a natural user interaction with digital information. Embodied interaction makes an enormous shift from previous user interaction paradigm. Moving from time to space, it takes the interaction “off the screen” into the real world [@Dourish:2001vc]; distributing inputs in space, de-sequentialising interaction and reducing the gap between where the information is created where it is accessed. The interactional media, with its affordances, is the interface: “By treating the body of the device as part of the user interface —an embodied user interface— we can go beyond the manipulation of a GUI and allow the user to really directly manipulate an integrated physical-virtual device” [@Fishkin:2000df] 

*Tangible user interfaces* are computer interfaces in which technology is embedded into physical objects and spaces, enabling an *embodied interaction* with digital information. In this picture, unlike GUIs which manipulate virtual elements (e.g. icons) with the aid of keyboard and mouse, TUIs integrates both representation and control of computation into physical artefacts [@krumm2009ubiquitous]. This approach allows system designers to be free to experiment with new type of metaphors, taking advantage of users’ physical skills and providing interfaces which exploit people’s knowledge with the everyday, non-digital, world [@Jacob:2008vm]. Since the ways the manipulation of physical media profoundly differs from the manipulation of the digital [@Terrenghi:2007uv], metaphors adopted for the digital world need to be redesigned to meet physical affordances. Indeed, TUIs as well as other sensing-based interfaces poses new challenges to designers [@Bellotti:2002wg]. Those challenges were also further elaborated by extended by Marquardt and Greenberg [-@Marquardt:2012tg]. Several terms have been used to characterise systems of tangible interfaces, as tangible, graspables, tokens, containers, phicons, tangible bits; in the following I will call them tangibles. 

Tangibles are part of the broad field known as Ubiquitous Computing, widely attributed to the work of Mark Weiser. In his pioneering article on Scientific American in 1991 [@weiser1991computer] he envisioned a close coupling between the digital and the physical, to the extent that the technology “disappears into the fabric of the everyday life”.

### Frameworks

Over the recent years several research works have proposed frameworks either to characterise existing systems or to provide  opportunities and guidelines to support the design of new systems of tangibles. In 2000 Ullmer and Ishii [-@Ullmer:2000vf] took the first steps in investigating the design of tangibles. They defined TUIs as systems that give physical form to digital information, a research branded as “tangible bits” [@Ishii:2008fh]; and categorised them in spatial, relational, constructive and mixed systems. They presented a conceptual framework and interaction model called MCRit.  MCRit, an abbreviation for Model-Control-Representation (tangible and intangible), adapts the Model-View-Controller (MVC) model of GUI-based interaction to the design of tangibles. Besides MCRit highlights the “seamless integration of control and representation” characteristic of tangibles, it redefines the capability of the interface to provide information as a balance between its physical (the object’s shape and affordances) and a intangible representations (e.g. computer graphics and sounds)(Figure x). For example augmenting physical objects with video-projections and sounds in order to extend the static representation of an object with an intangible, dynamic one.

![Tangible and intangible representations of TUI, figure from [@Ishii:2008fh]](imgs/mcrit.png)

In fact one of the main pitfall of TUIs is that while 
GUIs serve as generic purpose interface by allowing multiple kinds of tasks, defined by the software, TUIs serve as special purposes interfaces, each one tailored on a specific set of 
actions (defined by physical affordances and constrains). A tangible interface can hardly be adapted to work in a context that differs from the one it has been designed for. It is what is defined as the trade-off between Reality and Versatility [@Jacob:2008vm]: it trades the capability of a system of doing many different tasks (like browsing photos, writing a document) with the possibility to accomplish only one single task with a higher level of realism or simplicity. 

Following the work of Ullmer and Ishii other works have looked at TUIs from other perspectives. Jacob et al. [-@Jacob:2008vm] identify four themes of interaction with the real world that can be leveraged for the design of TUIs. Hornecker and Buur [-@Hornecker:2006uq] present four topics to be considered in scenarios where tangible interaction have social aspects. Fishking [@Fishkin:2004uv] presents an aggregated perspective on other frameworks categorising tangible systems as a continuum of level of embodiment and metaphor they provide. Finally Ullmer et al. envision TUIs as a systems of *tokens* and *constraints*. The former are discrete physical objects that represent digital information, the latter mechanical or visual confining regions that are mapped to digital operations. By the interaction phases of association and manipulation of tokens within a system of constraints it is possible to map physical actions to a set of computational operations in a grammar of ways; for example the presence or absence of a token in a constrained area could be easily digitalised in binary information to trigger a digital operation. For a literature of other frameworks see [@Mazalek:2009uy] and [@Shaer:2009fx]. 

### Applications in experiential learning

Although the relation between tangibles and experiential learning hasn’t been thoroughly investigated, several works have shown that TUIs might be beneficial for learning [@Marshall:2007dr]; for a review see [@omalley:hal-00190328]. Despite those works often focus on applications of TUIs for children and classroom environments, there are evidences that TUIs have possible benefits on learning on a broader scope. Those benefits include the support to more natural [@Terrenghi:2005gq] and situated [@Klemmer:2006ez] learning , increased reflection and engagement [@Rogers:2006te] due to the link between physical action and digital feedbacks. Moreover TUIs foster collaboration [@Rogers:2003tt], in which they increase visibility of others’ actions and allow for concurrent interaction.

Theoretical tools adapted from the presented frameworks have driven the design of embodied interaction systems in P2, P3, P4, P6 described in ChapterX, they are one of the results of this PhD work. Next, I will present the research design for this research that have lead to the results.


