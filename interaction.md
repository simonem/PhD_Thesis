# Sensing-based interaction

The design of systems to support reflection on crisis work experiences benefits from research in the field of HCI and sensing-based interaction [@Benford:2005bo]. Sensing-based interfaces is rather a broad term referring to user interfaces that rely on sensor technology to make interaction between people and computers more intuitive and effective [@Zhai:2005jm]. Sensor interfaces allows for post-WIMP [@VanDam:1997tz] interaction paradigms, not relying on traditional **W**indows, **I**cons, **M**ouse and **P**ointers. Instead, sensing-based interfaces promote “embodied interaction, tangible manipulation, physical representation of data and embeddedness in real space” [@Hornecker:2006uq]. 

According with Rogers and Muller [-@Rogers:2006te] sensing-based interaction allows for designing systems capable to deliver relevant information at appropriate times, which is critical to trigger and sustain reflection; and enable “hands-free control”, which is fundamental for unobtrusively capturing data in action. *Embodied* and *Tangible* are characterising tracts of sensing-based interfaces that will be investigated in the next section.

## Tangible interfaces and Embodied interaction

Embodied interaction, as defined by Dourish [@Dourish:2001vc], is a collection of a number of recent trends emerged in HCI, relying on the common ground to provide a natural, native user interaction with digital information. Embodied interaction makes an enormous shift in the user interaction paradigm. Moving from time to space, it takes the interaction “off the screen” into the real world [@Dourish:2001vc]; distributing inputs in space, de-sequentialising interaction and reducing the gap between where the information is created where it is accessed. The interaction media, with its affordances is the interface: “By treating the body of the device as part of the user interface —an embodied user interface— we can go beyond the manipulation of a GUI and allow the user to really directly manipulate an integrated physical-virtual device” [@Fishkin:2000df] 

Tangible user interfaces are computer interfaces in which technology disappears into physical objects and spaces [@Weiser:1993wy], enabling an *embodied interaction* with digital information. In this picture, unlike GUIs which manipulate virtual elements (e.g. icons) with the aid of keyboard mouse, TUIs integrates both representation and control of computation into physical artefacts [@krumm2009ubiquitous]. This approach allows system designers to be free to experiment with new type of metaphors, taking advantage of users’ physical skills and providing interfaces which exploit people’s knowledge with the everyday, non-digital, world [@Jacob:2008vm]. Since the ways the manipulation of physical media profoundly differs from the manipulation of the digital [@Terrenghi:2007uv], metaphors adopted for the digital world need to be redesigned to meet physical affordances. Indeed, TUIs as well as other sensing-based interfaces poses new challenges to designers [@Bellotti:2002wg].   

### Frameworks

Over the recent years several research work have proposed frameworks either to characterise existing systems or to provide  opportunities and guidelines to support the design of new systems of tangibles. In 2000 Ullmer and Ishii [-@Ullmer:2000vf] took the first steps in investigating the design of TUIs by presenting a conceptual framework and interaction model called MCRit. They defined TUIs as systems that give physical form to digital information, a research later branded as “tangible bits” [Ishii:2008fh]; to employ physical artefacts both as representation and control of digital information. MCRit, an abbreviation for Model-Control-Representation (tangible and intangible), adapts the Model-View-Controller (MVC) model of GUI-based interaction to the design of TUIs. Besides MCRit highlights the “seamless integration of control and representation” of TUIs, it redefines the capability of the interface to provide information as a balance between its physical (the object’s shape and affordances) and a intangible representation (e.g. computer graphics and sounds)(Figure x). For example augmenting physical objects with video-projections and sounds in order to extend the static representation of an object with an intangible, dynamic one.

![Tangible and intangible representations of TUI, figure from [Ishii:2008fh]](imgs/mcrit.png)

In fact one of the main pitfall of TUIs is that while 
GUIs serve as generic purpose interface by allowing multiple kinds of tasks, defined by the software, TUIs serve as special purposes interfaces, each one tailored on a specific set of 
actions (defined by physical affordances and constrains). A tangible interface can hardly be adapted to work in a context that differs from the one the interface has been designed for. It is what is defined as the trade-off between Reality and Versatility [@Jacob:2008vm]: it trades the capability of a system of doing many different tasks (like browsing photos, writing a document) with the possibility to accomplish only one single task with a higher level of realism or simplicity. 

Following the work of Ullmer and Ishii other works have looked at TUIs from other perspectives. Jacob et al. [-@Jacob:2008vm] identify four themes of interaction with the real world that can be leveraged for the design of TUIs. Hornecker and Buur [-@Hornecker:2006uq] present four topics to be considered in scenarios where tangible interaction have social aspects. Fishking [@Fishkin:2004uv] presents and aggregated perspective on other frameworks creating a for categorising tangible systems according to the level of embodiment and metaphor they provide. Finally Ullmer et al. envision TUIs as a systems of Tokens, discrete physical objects that represent digital information; and Constraints, mechanical or visual confining regions that are mapped to digital operations. By the interaction phases of association and manipulation of tokens within a system of constraints it is possible to map physical actions to a set of computational operations; for example the presence or absence of a token in a constrained area could be easily digitalised in binary information. For a literature of other frameworks see [@Mazalek:2009uy] and [@Shaer:2009fx]. 

The design of user interfaces in P2, P3, P4, P6 deeply rely on theoretical tools adapted from the presented frameworks. The design of the system presented in P1, which is based on mobile augmented reality, can be seen itself as an embodied interface since it requires the user to physically move the interface in the physical space in order to interact with the system.

Although several ones have focused on TUIs for learning [@Markova:2012js]  





## Wearable computing, Augmented Reality and Token-based interaction

touchscreen as a bottleneck of interaction

## Rapid prototyping, tools and techniques adopted
### Software prototyping 
### Hardware prototyping
### System integration and the cloud