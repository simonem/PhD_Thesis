\chapter{Evaluation}\label{evaluation}

\todo[inline]{Revision 3}

This chapter provides an evaluation of the contributions of the thesis with respect to research questions. In addition, validity threats are discussed in the chapter.

\section{Evaluation of research questions}\label{evaluation-of-research-questions}

\subsection{MRQ: \MRQ}\label{mrq}

The main research question is answered by Contribution 1. Using crisis training as case study for design research, it is demonstrated how conceptual tools from theory in CSRL can be implemented in a suite of applications of sensing-based technology: \emph{WATCHiT}, \emph{CroMAR}, and \emph{Don't Panic}. Each application assists specific activities which the CSRL model has identified to be relevant for reflection. Two or more applications can be configured to work together in an ecology, in order to address the interrelated nature of CSRL activities in terms of sharing of data and reflection outcomes. In this way it is also addressed the need for supporting the extremely vast range of scenarios workers train for by means of loosely coupled, modular applications.

\subsection{RQ1: \RQi}\label{rq1}

This question is answered by Contribution 2 and Contribution 3. C2 has identified a set of challenges for the design of experience-capturing tools, focusing on \emph{what} data to collect and \emph{how} to collect it. One of the identified challenges, the need of intuitive and hands-free user interfaces for capturing user-submitted qualitative information, has been further addressed by C3 with the design of a novel user interface for controlling the data capture process.

\subsection{RQ2: \RQii}\label{rq2}

This question is answered by Contribution 3. It is found that theories in the field of tangible, embodied and embedded computing can drive the design of sensing-based interfaces either to \emph{capture work experience} and to \emph{generate work experience}. The specific paradigms adopted are mobile augmented reality, body-centric interaction and token+constraint interaction. Once more it is not a single interaction modality that has been proven to engage reflection but rather a mix of different technology-assisted experience.

\subsection{RQ3: \RQiii}\label{rq3}

This question is answered by Contribution 4. It is found out that prototyping sensing-based interfaces for supporting reflection require a wide range of skills ranging from software and electronic engineering. Yet a toolbox for assisting in full the work of designers and engineers in implementing functional requirements in prototypes could not be found. Despite that, recent advances in open source hardware and software, digital manufacturing and technology developed by the author (P7) allowed the author to build working prototypes robust enough to undergo evaluations during simulated crisis work. Still the production of prototypes requires resources and skills not usually required for prototyping ``traditional'' ICT systems.

\section{Evaluation of research approach}\label{evaluation-of-research-approach}

The research approach taken in this thesis has a number of limitations. In this section validity issues are discussed.

\subsection{Conclusion validity}\label{conclusion-validity}

Conclusion validity questions whether the method used shows a relationship between the variables that are studied.

In this work the impact on reflective learning for the technology tools developed was evaluated (as proof of validity for C1). Also due to the novel nature of the user interfaces developed, technology acceptance tests and usability evaluations have been performed.

Considering the design research approach taken and the number of human factors involved, methods from qualitative research were used in the evaluations. The methods used included observations, semi-structured interviews, surveys and questionaries. The adoption of the MIRROR Evaluation Toolbox \autocite{Renner:v4nLmwOk}, which provided questionnaires to measure reflective learning at work, straighten the conclusion validity of the research. These questionnaires are built on the Kirkpatrick framework \autocite{kirkpatrick2009evaluating} and have been developed through an extensive survey of literature on reflective learning and in cooperation with participants from different workplace settings.

The number of participants in the studies added up to 56 crisis field workers (e.g.~firefighters, paramedics), 1 disaster manager, 8 IT students and 4 HCI experts. Among the field workers 5 participants attended more than one study. The size of data samples used for statistical analysis is therefore limited and it weakens the conclusion validity, especially for the evaluation studies for the applications targeting disaster managers (CroMAR). Furthermore most of evaluation studies lasted only for a few hours. It is therefore not possible to assess the long-term learning effects for the solutions developed.

\subsection{Internal validity}\label{internal-validity}

Internal validity questions whether the method used demonstrates a casual relationship between two variables or not.

By running field studies during physical simulations arranged by external organisations, most variables could not be controlled. Access to workers and locations where the work is performed was restricted to avoid obstructing activities, explosing training objectives of the events and attendees' safety at risk. Despite the goodwill of simulations' attendees and organisers, it was not always possible to fully attend to the evaluation protocol agreed due to both planned and unplanned unpredictability of such events.

Yet the lack of control was traded for realism. Events attended recreated working conditions as close as possible to real emergencies. Participants were unaware of the type of emergency to face until the very last moment, human mistakes and failure of technical systems were re-created on purpose by the event manager. Finally, during the observed events, volunteers acting as injured were made-up with fake wounds and instructed to behave accordingly (e.g.~panicking). Considering the very situated nature of crisis work, the widespread layout of activities in space, the coexistence of multiple roles and organisations, choosing field studies over an experimental strategy has strengthen the internal validity of the work. This is necessary as running field studies during real crisis work is not feasible due to safety reasons.

\subsection{External validity}\label{external-validity}

External validity questions whether the relationship found can be generalised to other settings.

Given the characteristics of the sample of population the research has been evaluated with, it is not possible to draw conclusions about generalisations of results, both within the crisis domain and for other settings.

First, participants of field studies performed are all affiliated to a small set of crisis response organisations; the design research has been case studied on the specific needs of those organisations. Although a review of literature have shown that technology requirements found in our user studies are common to other organisations in different countries, it is not given to know how the results from this work could be accepted by other organisations. This weakens the results of research. Contacts with external organisations have been established in order to evaluate generalisation of results as part of future work.

Second, it has not been thoroughly investigated how results from this PhD could be translated to work practices that are different than the crisis domain. The novel interaction techniques and the rapid prototyping approach developed have been employed to create games for training workers in dementia care homes for better care, during a research visit abroad. The same approach drove the design and implementation of tangible interface to promote user engagement and reflection about urban-mobility data, as part of a second research visit abroad (Appendix \ref{abroad})

\subsection{Reliability}\label{reliability}

Reliability is the extent to which another researcher would find the same answers.

Most of the data were only observed, recorded and analysed by one person, which is subject to interpretation bias. However some of the data were analysed by two or more people. Further, different researchers have been responsible for different studies. 
